{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.KeyedVectors object at 0x7f0c05858e48>\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model_file = 'models/cn.cbow.bin'\n",
    "#model = gensim.models.Word2Vec.load(model_file,binary=True)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_file, binary=True,unicode_errors='ignore')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on KeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class KeyedVectors(gensim.utils.SaveLoad)\n",
      " |  Class to contain vectors and vocab for the Word2Vec training class and other w2v methods not directly\n",
      " |  involved in training such as most_similar()\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Accept a single word or a list of words as input.\n",
      " |      \n",
      " |      If a single word: returns the word's representations in vector space, as\n",
      " |      a 1D numpy array.\n",
      " |      \n",
      " |      Multiple words: return the words' representations in vector space, as a\n",
      " |      2d numpy array: #words x #vector_size. Matrix rows are in the same order\n",
      " |      as in input.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model['office']\n",
      " |        array([ -1.40128313e-02, ...])\n",
      " |      \n",
      " |        >>> trained_model[['office', 'products']]\n",
      " |        array([ -1.40128313e-02, ...]\n",
      " |              [ -1.70425311e-03, ...]\n",
      " |               ...)\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function KeyedVectors.most_similar at 0x7f0bd7036158>, case_insensitive=True)\n",
      " |      Compute accuracy of the model. `questions` is a filename where lines are\n",
      " |      4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |      See questions-words.txt in https://storage.googleapis.com/google-code-archive-source/v2/code.google.com/word2vec/source-archive.zip for an example.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all questions containing a word not in the first `restrict_vocab`\n",
      " |      words (default 30,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      In case `case_insensitive` is True, the first `restrict_vocab` words are taken first, and then\n",
      " |      case normalization is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in questions and vocab to their uppercase form before\n",
      " |      evaluating the accuracy (default True). Useful in case of case-mismatch between training tokens\n",
      " |      and question words. In case of multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
      " |        'cereal'\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments. `pairs` is a filename of a dataset where\n",
      " |      lines are 3-tuples, each consisting of a word pair and a similarity value, separated by `delimiter'.\n",
      " |      An example dataset is included in Gensim (test/test_data/wordsim353.tsv). More datasets can be found at\n",
      " |      http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html or https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      The model is evaluated using Pearson correlation coefficient and Spearman rank-order correlation coefficient\n",
      " |      between the similarities from the dataset and the similarities produced by the model itself.\n",
      " |      The results are printed to log and returned as a triple (pearson, spearman, ratio of pairs with unknown words).\n",
      " |      \n",
      " |      Use `restrict_vocab` to ignore all word pairs containing a word not in the first `restrict_vocab`\n",
      " |      words (default 300,000). This may be meaningful if you've sorted the vocabulary by descending frequency.\n",
      " |      If `case_insensitive` is True, the first `restrict_vocab` words are taken, and then case normalization\n",
      " |      is performed.\n",
      " |      \n",
      " |      Use `case_insensitive` to convert all words in the pairs and vocab to their uppercase form before\n",
      " |      evaluating the model (default True). Useful when you expect case-mismatch between training tokens\n",
      " |      and words pairs in the dataset. If there are multiple case variants of a single word, the vector for the first\n",
      " |      occurrence (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Use `dummy4unknown=True' to produce zero-valued similarities for pairs with out-of-vocabulary words.\n",
      " |      Otherwise (default False), these pairs are skipped entirely.\n",
      " |  \n",
      " |  get_embedding_layer(self, train_embeddings=False)\n",
      " |      Return a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      If `replace` is set, forget the original vectors and only keep the normalized\n",
      " |      ones = saves lots of memory!\n",
      " |      \n",
      " |      Note that you **cannot continue training** after doing a replace. The model becomes\n",
      " |      effectively read-only = you can call `most_similar`, `similarity` etc., but not `train`.\n",
      " |  \n",
      " |  most_similar(self, positive=[], negative=[], topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words. Positive words contribute positively towards the\n",
      " |      similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      If topn is False, most_similar returns the vector of similarity scores.\n",
      " |      \n",
      " |      `restrict_vocab` is an optional integer which limits the range of vectors which\n",
      " |      are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |      only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |      meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
      " |        [('queen', 0.50882536), ...]\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=[], negative=[], topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective\n",
      " |      proposed by Omer Levy and Yoav Goldberg in [4]_. Positive words still contribute\n",
      " |      positively towards the similarity, negative words negatively, but with less\n",
      " |      susceptibility to one large distance dominating the calculation.\n",
      " |      \n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively – a potentially sensible but untested extension of the method. (With\n",
      " |      a single positive example, rankings will be the same as in the default most_similar.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.most_similar_cosmul(positive=['baghdad', 'england'], negative=['london'])\n",
      " |        [(u'iraq', 0.8488819003105164), ...]\n",
      " |      \n",
      " |      .. [4] Omer Levy and Yoav Goldberg. Linguistic Regularities in Sparse and Explicit Word Representations, 2014.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant'])\n",
      " |        0.61540466561049689\n",
      " |      \n",
      " |        >>> trained_model.n_similarity(['restaurant', 'japanese'], ['japanese', 'restaurant'])\n",
      " |        1.0000000000000004\n",
      " |      \n",
      " |        >>> trained_model.n_similarity(['sushi'], ['restaurant']) == trained_model.similarity('sushi', 'restaurant')\n",
      " |        True\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the object to file (also see `load`).\n",
      " |      \n",
      " |      `fname_or_handle` is either a string specifying the file name to\n",
      " |      save to, or an open file-like object which can be written to. If\n",
      " |      the object is a file handle, no special array handling will be\n",
      " |      performed; all attributes will be saved to the same file.\n",
      " |      \n",
      " |      If `separately` is None, automatically detect large\n",
      " |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      " |      them into separate files. This avoids pickle memory errors and\n",
      " |      allows mmap'ing large arrays back on load efficiently.\n",
      " |      \n",
      " |      You can also set `separately` manually, in which case it must be\n",
      " |      a list of attribute names to be stored in separate files. The\n",
      " |      automatic check is not performed in this case.\n",
      " |      \n",
      " |      `ignore` is a set of attribute names to *not* serialize (file\n",
      " |      handles, caches etc). On subsequent load() these attributes will\n",
      " |      be set to None.\n",
      " |      \n",
      " |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      " |      in both Python 2 and 3.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |       `fname` is the file used to save the vectors in\n",
      " |       `fvocab` is an optional file used to save the vocabulary\n",
      " |       `binary` is an optional boolean indicating whether the data is to be saved\n",
      " |       in binary word2vec format (default: False)\n",
      " |       `total_vec` is an optional parameter to explicitly specify total no. of vectors\n",
      " |       (in case word vectors are appended with document vectors afterwards)\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      If topn is False, similar_by_vector returns the vector of similarity scores.\n",
      " |      \n",
      " |      `restrict_vocab` is an optional integer which limits the range of vectors which\n",
      " |      are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |      only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |      meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similar_by_vector([1,2])\n",
      " |        [('survey', 0.9942699074745178), ...]\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      If topn is False, similar_by_word returns the vector of similarity scores.\n",
      " |      \n",
      " |      `restrict_vocab` is an optional integer which limits the range of vectors which\n",
      " |      are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |      only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |      meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similar_by_word('graph')\n",
      " |        [('user', 0.9999163150787354), ...]\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model.similarity('woman', 'man')\n",
      " |        0.73723527\n",
      " |      \n",
      " |        >>> trained_model.similarity('woman', 'woman')\n",
      " |        1.0\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents. When using this\n",
      " |      code, please consider citing the following papers:\n",
      " |      \n",
      " |      .. Ofir Pele and Michael Werman, \"A linear time histogram metric for improved SIFT matching\".\n",
      " |      .. Ofir Pele and Michael Werman, \"Fast and robust earth mover's distances\".\n",
      " |      .. Matt Kusner et al. \"From Word Embeddings To Document Distances\".\n",
      " |      \n",
      " |      Note that if one of the documents have no words that exist in the\n",
      " |      Word2Vec vocab, `float('inf')` (i.e. infinity) will be returned.\n",
      " |      \n",
      " |      This method only works if `pyemd` is installed (can be installed via pip, but requires a C compiler).\n",
      " |      \n",
      " |      Example:\n",
      " |          >>> # Train word2vec model.\n",
      " |          >>> model = Word2Vec(sentences)\n",
      " |      \n",
      " |          >>> # Some sentences to test.\n",
      " |          >>> sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
      " |          >>> sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
      " |      \n",
      " |          >>> # Remove their stopwords.\n",
      " |          >>> from nltk.corpus import stopwords\n",
      " |          >>> stopwords = nltk.corpus.stopwords.words('english')\n",
      " |          >>> sentence_obama = [w for w in sentence_obama if w not in stopwords]\n",
      " |          >>> sentence_president = [w for w in sentence_president if w not in stopwords]\n",
      " |      \n",
      " |          >>> # Compute WMD.\n",
      " |          >>> distance = model.wmdistance(sentence_obama, sentence_president)\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Accept a single word as input.\n",
      " |      Returns the word's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      If `use_norm` is True, returns the normalized word vector.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |        >>> trained_model['office']\n",
      " |        array([ -1.40128313e-02, ...])\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Note that the information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      `binary` is a boolean indicating whether the data is in binary word2vec format.\n",
      " |      `norm_only` is a boolean indicating whether to only store normalised word2vec vectors in memory.\n",
      " |      Word counts are read from `fvocab` filename, if set (this is the file generated\n",
      " |      by `-save-vocab` flag of the original C tool).\n",
      " |      \n",
      " |      If you trained the C model using non-utf8 encoding for words, specify that\n",
      " |      encoding in `encoding`.\n",
      " |      \n",
      " |      `unicode_errors`, default 'strict', is a string suitable to be passed as the `errors`\n",
      " |      argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |      file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |      (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      \n",
      " |      `limit` sets a maximum number of word-vectors to read from the file. The default,\n",
      " |      None, means read all.\n",
      " |      \n",
      " |      `datatype` (experimental) can coerce dimensions to a non-default float type (such\n",
      " |      as np.float16) to save memory. (Such types may result in much slower bulk operations\n",
      " |      or incompatibility with optimized routines.)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  load(fname, mmap=None) from builtins.type\n",
      " |      Load a previously saved object from file (also see `save`).\n",
      " |      \n",
      " |      If the object was saved with large arrays stored separately, you can load\n",
      " |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      " |      mmap, load large arrays as normal objects.\n",
      " |      \n",
      " |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      " |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      " |      is encountered.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31951836139991313"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('打开','启动')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2484268188294381"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('打开','开开')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58186725314911059"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('打开','开启')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51079443869428631"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('打开','关闭')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[ 0.01015623 -0.06191396 -0.07577638  0.104211    0.19398926 -0.03052631\n",
      "  0.01551417  0.03540653 -0.0481686   0.04433157]\n"
     ]
    }
   ],
   "source": [
    "res = model.word_vec('打开')\n",
    "print(res.shape)\n",
    "print(res[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.most_similar('打开')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57395405973206348"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['设置','红色'],['调到','红色'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57070989507846048"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['设置','红色'],['灯','调到','红色'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46185990104822416"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['我','想','吃饭'],['我','肚子','饿'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95362773486023389"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['我','想','吃','粥'],['我','要','吃','粥'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50887227301267846"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['我','想','吃','粥'],['我','肚子','饿'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13267149835522823"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_similarity(['我','想','吃','粥'],['灯','调到','红色'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
